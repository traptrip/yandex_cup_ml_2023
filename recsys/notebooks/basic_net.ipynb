{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Union, Optional, Tuple, List, Dict\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import models\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config: \n",
    "    # logging\n",
    "    log_dir = Path(\"exp/train\")\n",
    "    \n",
    "    # data\n",
    "    data_dir = Path(\"../data/\")\n",
    "    num_labels = 256\n",
    "    crop_size = 81\n",
    "\n",
    "    batch_size = 256\n",
    "    eval_batch_size = 256\n",
    "    num_workers = 4\n",
    "\n",
    "    # net\n",
    "    input_dim = 768\n",
    "    hidden_dim = 512\n",
    "\n",
    "    device = \"cuda:1\"\n",
    "    use_amp = True\n",
    "    clip_value = None\n",
    "    lr = 1e-3\n",
    "    min_lr = 1e-8\n",
    "\n",
    "    n_epochs = 20\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp/train_3\n"
     ]
    }
   ],
   "source": [
    "def get_exp_name(log_dir: Path):\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    new_exp_name = log_dir.name\n",
    "    prev_exps = [exp.name for exp in log_dir.parent.iterdir()]\n",
    "    last_exp_num = \"\"\n",
    "    for exp in prev_exps:\n",
    "        if new_exp_name in exp:\n",
    "            tmp = str(exp).split(\"_\")\n",
    "            if len(tmp) > 1: \n",
    "                last_exp_num = int(tmp[-1]) + 1\n",
    "            else:\n",
    "                last_exp_num = 1\n",
    "            last_exp_num = f\"_{last_exp_num}\"\n",
    "    return log_dir.parent / f\"{new_exp_name}{last_exp_num}\"\n",
    "\n",
    "logdir = get_exp_name(cfg.logs_dir)\n",
    "print(logdir)\n",
    "tb = SummaryWriter(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed98b2ab7fde43c296049ac63f759bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df_train = pd.read_csv('../data/train.csv')\n",
    "# df_test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "# track_idx2embeds = {}\n",
    "# for fn in tqdm(glob('../data/track_embeddings/*')):\n",
    "#     track_idx = int(fn.split('/')[3].split('.')[0])\n",
    "#     embeds = np.load(fn)\n",
    "#     track_idx2embeds[track_idx] = embeds\n",
    "\n",
    "# def collate_fn(b):\n",
    "#     track_idxs = torch.from_numpy(np.vstack([x[0] for x in b]))\n",
    "#     targets = torch.from_numpy(np.vstack([x[2] for x in b]))\n",
    "#     # embeds = [torch.from_numpy(x[1]) for x in b]\n",
    "#     embeds = torch.stack([x[1] for x in b])\n",
    "#     return track_idxs, embeds, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        num_labels=256,\n",
    "        crop_size=60,\n",
    "        stage=\"train\",\n",
    "        transform=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.stage = stage\n",
    "        self.transform = transform\n",
    "        self.crop_size = crop_size\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.meta_info = pd.read_csv(self.data_dir / \"metadata.csv\", sep=\"\\t\")\n",
    "\n",
    "        assert stage in (\"train\", \"val\", \"test\", \"infer\")\n",
    "\n",
    "        if self.stage in [\"train\", \"val\", \"test\"]:\n",
    "            if \"stage\" in self.meta_info.columns:\n",
    "                self.meta_info = self.meta_info.loc[self.meta_info.stage == stage].reset_index(drop=True)\n",
    "            self.labels = torch.tensor(self.meta_info.tags.apply(self.process_tags)).float()\n",
    "\n",
    "        self.tracks = self.meta_info.track.values\n",
    "\n",
    "        print(\"Uploading embeddings into memory\")\n",
    "        self.embeddings = [self.get_embedding(track) for track in self.tracks]\n",
    "\n",
    "    def process_tags(self, tags):\n",
    "        tags = list(map(int, tags.split(\",\")))\n",
    "        one_hot_tags = np.zeros(self.num_labels, dtype=np.uint8)\n",
    "        one_hot_tags[tags] = 1\n",
    "        return one_hot_tags.tolist()\n",
    "\n",
    "    def get_embedding(self, track: int) -> torch.Tensor:\n",
    "        embeddings = np.load(self.data_dir / f\"track_embeddings/{track}.npy\")\n",
    "        embeddings = torch.from_numpy(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "    def __process_features(self, x: torch.Tensor):\n",
    "        # normalize\n",
    "        # x /= x.max()\n",
    "        # x = (x - x.mean()) / x.std()\n",
    "\n",
    "        # add padding\n",
    "        x = x.permute(1, 0)\n",
    "        x_len = x.shape[-1]\n",
    "        if x_len > self.crop_size:\n",
    "            start = np.random.randint(0, x_len - self.crop_size)\n",
    "            x = x[..., start : start + self.crop_size]\n",
    "        else:\n",
    "            if self.stage == \"train\":\n",
    "                i = (\n",
    "                    np.random.randint(0, self.crop_size - x_len)\n",
    "                    if self.crop_size != x_len\n",
    "                    else 0\n",
    "                )\n",
    "            else:\n",
    "                i = (self.crop_size - x_len) // 2\n",
    "            pad_patern = (i, self.crop_size - x_len - i)\n",
    "            x = torch.nn.functional.pad(x, pad_patern, \"constant\").detach()\n",
    "        x = x.permute(1, 0)\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        track_features = self.embeddings[idx]\n",
    "        track_features = self.__process_features(track_features)\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "\n",
    "            out = {\n",
    "                \"features\": track_features,\n",
    "                \"label\": label,\n",
    "                \"track\": self.tracks[idx],\n",
    "            }\n",
    "            # return self.tracks[idx], track_features, label\n",
    "        else:\n",
    "            out = {\n",
    "                \"features\": track_features,\n",
    "                \"track\": self.tracks[idx],\n",
    "            }\n",
    "            # return self.tracks[idx], track_features\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_labels):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                input_dim, 8, dim_feedforward=2048, dropout=0.2, batch_first=True\n",
    "            ),\n",
    "            num_layers=3\n",
    "        )\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((1, input_dim))\n",
    "        self.projector =  nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x)\n",
    "        x = self.pooling(x).squeeze()\n",
    "        x = self.projector(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_dataloader, val_dataloader, n_epochs, optimizer, criterion, device, scheduler=None, use_amp=True):\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    alpha = 0.8\n",
    "\n",
    "    for epoch in range(n_epochs): \n",
    "        # Training\n",
    "        net.train()     \n",
    "        train_loss = None\n",
    "        train_targets = []\n",
    "        train_preds = []\n",
    "        for data in (pbar := tqdm(train_dataloader)):\n",
    "            optimizer.zero_grad()\n",
    "            track_ids, batch, targets = data[\"track\"], data[\"features\"], data[\"label\"]\n",
    "            # track_ids, batch, targets = data\n",
    "            batch = batch.to(device)\n",
    "            targets = targets.to(device)\n",
    "            with torch.cuda.amp.autocast(enabled = use_amp), torch.backends.cuda.sdp_kernel(enable_flash=cfg.device == \"cuda:0\"):\n",
    "                logits = net(batch)\n",
    "                loss = criterion(logits, targets)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            if cfg.clip_value is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), cfg.clip_value)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss = loss.item() if not train_loss else alpha * train_loss + (1 - alpha) * loss.item()\n",
    "            train_targets.extend(targets.cpu().numpy())\n",
    "            train_preds.extend(torch.sigmoid(logits.detach()).cpu().numpy())\n",
    "\n",
    "            pbar.set_description(f\"Epoch: {epoch} Loss: {train_loss:.6f}\")\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        train_loss = np.mean(train_loss)\n",
    "        train_score = average_precision_score(train_targets, train_preds)\n",
    "        print('Train Loss:', train_loss)\n",
    "        print('Train AP:', train_score)\n",
    "\n",
    "        # Evaluation\n",
    "        net.eval()   \n",
    "        val_loss = None\n",
    "        val_targets = []\n",
    "        val_preds = []\n",
    "        for data in (pbar := tqdm(val_dataloader)):\n",
    "            with torch.no_grad():\n",
    "                with torch.cuda.amp.autocast(enabled = use_amp), torch.backends.cuda.sdp_kernel(enable_flash=cfg.device == \"cuda:0\"):\n",
    "                    track_ids, batch, targets = data[\"track\"], data[\"features\"], data[\"label\"]\n",
    "                    # track_ids, batch, targets = data\n",
    "                    batch = batch.to(device)\n",
    "                    targets = targets.to(device)\n",
    "\n",
    "                    logits = net(batch)\n",
    "                    loss = criterion(logits, targets.float())\n",
    "\n",
    "                val_loss = loss.item() if not val_loss else alpha * val_loss + (1 - alpha) * loss.item()\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "                val_preds.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "\n",
    "                pbar.set_description(f\"Epoch: {epoch} Loss: {val_loss:.6f}\")\n",
    "\n",
    "        val_loss = np.mean(val_loss)\n",
    "        val_score = average_precision_score(val_targets, val_preds)\n",
    "        print('Val Loss:', val_loss)\n",
    "        print('Val AP:', val_score)\n",
    "\n",
    "        tb.add_scalar('Loss/train', train_loss, epoch)\n",
    "        tb.add_scalar('Loss/val', val_loss, epoch)\n",
    "        tb.add_scalar('AP/train', train_score, epoch)\n",
    "        tb.add_scalar('AP/val', val_score, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmbeddingDataset(cfg.data_dir, cfg.num_labels, cfg.crop_size, stage=\"train\")\n",
    "val_dataset = EmbeddingDataset(cfg.data_dir, cfg.num_labels, cfg.crop_size, stage=\"val\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=cfg.eval_batch_size, shuffle=False, num_workers=cfg.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(\n",
    "    input_dim=768,\n",
    "    hidden_dim=512,\n",
    "    num_labels=cfg.num_labels,\n",
    ").to(cfg.device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=cfg.lr)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.n_epochs, eta_min=cfg.min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1141f6bbd94ad48e15ae731fb4eb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/and/projects/hacks/yandex_cup_ml_2023/recsys/notebooks/basic_net.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bhom.rasdva3.com/home/and/projects/hacks/yandex_cup_ml_2023/recsys/notebooks/basic_net.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhom.rasdva3.com/home/and/projects/hacks/yandex_cup_ml_2023/recsys/notebooks/basic_net.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     net, train_loader, val_loader, cfg\u001b[39m.\u001b[39;49mn_epochs, optimizer, criterion, cfg\u001b[39m.\u001b[39;49mdevice, scheduler, cfg\u001b[39m.\u001b[39;49muse_amp\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhom.rasdva3.com/home/and/projects/hacks/yandex_cup_ml_2023/recsys/notebooks/basic_net.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m )\n",
      "\u001b[1;32m/home/and/projects/hacks/yandex_cup_ml_2023/recsys/notebooks/basic_net.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhom.rasdva3.com/home/and/projects/hacks/yandex_cup_ml_2023/recsys/notebooks/basic_net.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m cfg\u001b[39m.\u001b[39mclip_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhom.rasdva3.com/home/and/projects/hacks/yandex_cup_ml_2023/recsys/notebooks/basic_net.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(net\u001b[39m.\u001b[39mparameters(), cfg\u001b[39m.\u001b[39mclip_value)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bhom.rasdva3.com/home/and/projects/hacks/yandex_cup_ml_2023/recsys/notebooks/basic_net.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m scaler\u001b[39m.\u001b[39;49mstep(optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhom.rasdva3.com/home/and/projects/hacks/yandex_cup_ml_2023/recsys/notebooks/basic_net.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m scaler\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhom.rasdva3.com/home/and/projects/hacks/yandex_cup_ml_2023/recsys/notebooks/basic_net.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# loss.backward()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhom.rasdva3.com/home/and/projects/hacks/yandex_cup_ml_2023/recsys/notebooks/basic_net.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# optimizer.step()\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ya/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:370\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    368\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 370\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_opt_step(optimizer, optimizer_state, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    372\u001b[0m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m OptState\u001b[39m.\u001b[39mSTEPPED\n\u001b[1;32m    374\u001b[0m \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/miniforge3/envs/ya/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:289\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_maybe_opt_step\u001b[39m(\u001b[39mself\u001b[39m, optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    288\u001b[0m     retval \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39msum\u001b[39;49m(v\u001b[39m.\u001b[39;49mitem() \u001b[39mfor\u001b[39;49;00m v \u001b[39min\u001b[39;49;00m optimizer_state[\u001b[39m\"\u001b[39;49m\u001b[39mfound_inf_per_device\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues()):\n\u001b[1;32m    290\u001b[0m         retval \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mstep(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    291\u001b[0m     \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/miniforge3/envs/ya/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:289\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_maybe_opt_step\u001b[39m(\u001b[39mself\u001b[39m, optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    288\u001b[0m     retval \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39msum\u001b[39m(v\u001b[39m.\u001b[39;49mitem() \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    290\u001b[0m         retval \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mstep(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    291\u001b[0m     \u001b[39mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(\n",
    "    net, train_loader, val_loader, cfg.n_epochs, optimizer, criterion, cfg.device, scheduler, cfg.use_amp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
